# LiteLLM Documentation: https://docs.litellm.ai/docs/providers/vertex
model_list:
- model_name: google/gemini-2.0-flash-001
  litellm_params:
    model: vertex_ai/gemini-2.0-flash-001
- model_name: google/gemini-2.0-flash-lite-001
  litellm_params:
    model: vertex_ai/gemini-2.0-flash-lite-001
- model_name: google/gemini-2.0-flash-lite-001
  litellm_params:
    model: vertex_ai/gemini-2.0-flash-lite-001
- model_name: google/gemini-1.5-pro-002
  litellm_params:
    model: vertex_ai/gemini-1.5-pro-002
# Best practices for production
# Documentation: https://docs.litellm.ai/docs/proxy/prod
litellm_settings:
  telemetry: false
  set_verbose: false
  json_logs: true
  # Some vertex_ai models (like Llama 3.1) does not support parameters like: {'temperature': 0.2, 'top_p': 0.9}
  drop_params: true
  num_retries: 3 # retry call 3 times on each model_name
  request_timeout: 15 # raise Timeout error if call takes longer than 15s.
